{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART 1 Installing all packages (Set up IAM andconfigure AWS if not already done)\n",
    "!pip3 install sagemaker\n",
    "!pip3 install boto3\n",
    "!pip3 install json\n",
    "!pip3 install transformer\n",
    "!pip3 install os\n",
    "\n",
    "#Before starting: need to request more quotas for \"ml.m5.xlarge for transform job usage\", \"ml.m5.xlarge for endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART 2 getting Sagemaker running and getting personal info\n",
    "import sagemaker, boto3, json\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "aws_role = get_execution_role()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART 3 connecting specific HuggingFace Model to notebook\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "# Hugging Face Model configuration. https://huggingface.co/models\n",
<<<<<<< HEAD
    "    #if not working, hit deploy through sageMaker : https://huggingface.co/mdarhri00/named-entity-recognition \n",
=======
>>>>>>> c1e0b416b44f6bd931a3d6ff1abae4e906ff9782
    "hub = {\n",
    "    'HF_MODEL_ID': 'mdarhri00/named-entity-recognition',\n",
    "    'HF_TASK': 'token-classification'\n",
    "}\n",
    "\n",
    "# Create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    transformers_version='4.26.0',\n",
    "    pytorch_version='1.13.1',\n",
    "    py_version='py39',\n",
    "    env=hub,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "# Deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,  # Number of instances\n",
    "    instance_type='ml.m5.xlarge'  # EC2 instance type\n",
    ")\n",
    "\n",
    "# Make a prediction using the deployed model\n",
    "predictor.predict({\n",
<<<<<<< HEAD
    "    \"inputs\": \"predictor.predict({\n",
    "    \"inputs\": \"My name is Sarah Jessica Parker but you can call me Jessica and I live in College Park.\",\n",
    "})\n",
    "â€œ\n",
    "\n",
    "\n",
    "\n",
    "#choppy results that will be joined together in later code\n",
    "\n",
    "#Note for error regarding (ResourceLimitExceeded) you have to go to SageMaker and search \"increase quota\" then search \"ml.m5.xlarge\" and request more quotas each time. Then re-run the code."
=======
    "    \"inputs\": \"My name is Sarah Jessica Parker but you can call me Jessica and I live in College Park\",\n",
    "})\n",
    "\n",
    "#choppy results that will be joined together in later code\n",
    "\n",
    "#NOTE for error regarding (ResourceLimitExceeded) you have to go to SageMaker and search \"increase quota\" then search \"ml.m5.xlarge\" and request more quotas each time. Then re-run the code."
>>>>>>> c1e0b416b44f6bd931a3d6ff1abae4e906ff9782
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART 4 calling model name \n",
    "import boto3\n",
    "\n",
    "client = boto3.client('sagemaker')\n",
    "response = client.list_models(\n",
    ")\n",
    "\n",
    "for model in response['Models']:\n",
    "    print(model['ModelName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART 5 retrieving files from S3 and getting results\n",
    "import boto3\n",
    "import jsonlines\n",
    "\n",
    "#setting confidence minimun at 80%\n",
    "def extract_consecutive_entities_from_content(content, threshold=0.8):\n",
    "    entities = jsonlines.loads(content).get('entities', [])\n",
    "    #setting dictionary for Person and Location\n",
    "    consecutive_entities = {'PERSON': [], 'LOCATION': []}\n",
    "    current_entity_type = None\n",
    "    current_entity_score = None\n",
    "    current_entity_text = None\n",
    "\n",
    "    for entity in entities:\n",
<<<<<<< HEAD
    "        entity_type = entity['type']  #person or location\n",
    "        entity_score = entity['score']  #confidence\n",
    "        entity_text = entity['text']    \n",
    "#if two words have the same entity type, are above the thershold, and are next to each other in index (from example Sarah Jessica Parker) , but should still process individual names \n",
=======
    "        entity_type = entity['type']\n",
    "        entity_score = entity['score']\n",
    "        entity_text = entity['text']\n",
    "\n",
>>>>>>> c1e0b416b44f6bd931a3d6ff1abae4e906ff9782
    "        if (\n",
    "            current_entity_type == entity_type\n",
    "            and current_entity_score > threshold\n",
    "            and entity_score > threshold\n",
    "            and entity['index'] == current_entity_index + 1\n",
    "        ):\n",
<<<<<<< HEAD
    "            # Consecutive entity with the same type and score above threshold (concatinate into its own value in the dictionary)\n",
=======
    "            # Consecutive entity with the same type and score above threshold\n",
>>>>>>> c1e0b416b44f6bd931a3d6ff1abae4e906ff9782
    "            current_entity_text += ' ' + entity_text\n",
    "        else:\n",
    "            # Start a new consecutive sequence\n",
    "            consecutive_entities[entity_type].append(current_entity_text)\n",
    "            current_entity_type = entity_type\n",
    "            current_entity_score = entity_score\n",
    "            current_entity_text = entity_text\n",
    "            current_entity_index = entity['index']\n",
    "\n",
    "    # Add the last consecutive entity\n",
    "    if current_entity_type:\n",
    "        consecutive_entities[current_entity_type].append(current_entity_text)\n",
    "\n",
    "    return consecutive_entities\n",
    "\n",
    "def process_files_from_s3(bucket, prefix, threshold=0.8):\n",
    "    s3_client = boto3.client('s3')\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "    for obj in response.get('Contents', []):\n",
    "        file_key = obj['Key']\n",
    "        \n",
    "        # Read the content directly from S3\n",
<<<<<<< HEAD
    "        response = s3_client.get_object(Bucket=bucket, Key=file_key) #change bucket to bucketname\n",
=======
    "        response = s3_client.get_object(Bucket=bucket, Key=file_key)\n",
>>>>>>> c1e0b416b44f6bd931a3d6ff1abae4e906ff9782
    "        content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "        consecutive_entities = extract_consecutive_entities_from_content(content, threshold)\n",
    "\n",
    "        print(f\"Consecutive Entities extracted from {file_key}:\")\n",
    "        print(\"Person Names:\", consecutive_entities['PERSON'])\n",
    "        print(\"Locations:\", consecutive_entities['LOCATION'])\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Replace 'your-s3-bucket' and 'your-output-prefix' with your S3 bucket and prefix\n",
    "process_files_from_s3(bucket='your-s3-bucket', prefix='your-output-prefix')\n"
   ]
<<<<<<< HEAD
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
=======
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART 6\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
>>>>>>> c1e0b416b44f6bd931a3d6ff1abae4e906ff9782
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
